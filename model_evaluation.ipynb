{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new file named model_evaluation.py or model_evaluation.ipynb for these exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Given the following confusion matrix, evaluate (by hand) the model's performance.\n",
    "\n",
    "\n",
    "|               | pred dog   | pred cat   |\n",
    "|:------------  |-----------:|-----------:|\n",
    "| actual dog    |         46 |         7  |\n",
    "| actual cat    |         13 |         34 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that a dog is 0 (negative) and a cat is 1 (positive):\n",
    "\n",
    "|                | pred dog (0)| pred cat (1)|\n",
    "|:-------------  |------------:|------------:|\n",
    "| actual dog (0) |          46 |          7  |\n",
    "| actual cat (1) |          13 |          34 |\n",
    " \n",
    "- In the context of this problem, what is a false positive?\n",
    "    - FP = actual dog and model predicted cat: 7\n",
    "- In the context of this problem, what is a false negative?\n",
    "    - FN = actual cat and model predicted dog: 13\n",
    "- How would you describe this model?\n",
    "    - True Negative is actual dog and model predicted dog: 46\n",
    "    - False Positive is actual dog and model predicted cat: 7\n",
    "    - False Negative is actual cat and model predicted dog: 13\n",
    "    - True Positive is actual cat and model predicted cat: 34\n",
    "    - 80% accuracy\n",
    "    - 72% recall\n",
    "    - 83% precision\n",
    "    - 77% f-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives 34\n",
      "False Positives 7\n",
      "False Negatives 13\n",
      "True Negatives 46\n",
      "-------------\n",
      "Accuracy is, 0.8 or 80.00%\n",
      "Recall is,  0.72 or 72.34%\n",
      "Precision is,  0.83 or 82.93%\n",
      "F-1 score is,  0.77 or 77.27%\n"
     ]
    }
   ],
   "source": [
    "tn = 46\n",
    "fp = 7\n",
    "fn = 13\n",
    "tp = 34\n",
    "\n",
    "print(\"True Positives\", tp)\n",
    "print(\"False Positives\", fp)\n",
    "print(\"False Negatives\", fn)\n",
    "print(\"True Negatives\", tn)\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "accuracy = (tp+tn) / (tp+tn+fp+fn)\n",
    "recall = tp / (tp+fn)\n",
    "precision = tp / (tp+fp)\n",
    "f1_score = (2* precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"Accuracy is, {accuracy} or{accuracy: .2%}\")\n",
    "print(f\"Recall is, {recall: .2} or{recall: .2%}\")\n",
    "print(f\"Precision is, {precision: .2} or{precision: .2%}\")\n",
    "print(f\"F-1 score is, {f1_score: .2} or{f1_score: .2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. You are working as a datascientist working for Codeup Cody Creator (C3 for short), a rubber-duck manufacturing plant.\n",
    "\n",
    "Unfortunately, some of the rubber ducks that are produced will have defects. Your team has built several models that try to predict those defects, and the data from their predictions can be found here https://ds.codeup.com/data/c3.csv.\n",
    "\n",
    "Use the predictions dataset and pandas to help answer the following questions:\n",
    "\n",
    "- An internal team wants to investigate the cause of the manufacturing defects. They tell you that they want to identify as many of the ducks that have a defect as possible. \n",
    "    - Which evaluation metric would be appropriate here?\n",
    "        - Recall in order to avoid false negatives\n",
    "    - Which model would be the best fit for this use case?\n",
    "        - Model 3 has highest recall\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c3df = pd.read_clipboard()\n",
    "c3df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    184\n",
       "Defect        16\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#look at classes\n",
    "c3df.actual.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model1</th>\n",
       "      <th>Defect</th>\n",
       "      <th>No Defect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Defect</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No Defect</th>\n",
       "      <td>2</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model1     Defect  No Defect\n",
       "actual                      \n",
       "Defect          8          8\n",
       "No Defect       2        182"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#visualize actual vs prediction(model1)\n",
    "pd.crosstab(c3df.actual, c3df.model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No Defect    190\n",
       "Defect        10\n",
       "Name: model1, dtype: int64"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking value counts for model 1\n",
    "c3df.model1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8,   8],\n",
       "       [  2, 182]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#double check to see if array matches value counts\n",
    "model1 = confusion_matrix(c3df.actual, \n",
    "                          c3df.model1, \n",
    "                          labels=['Defect', 'No Defect'])\n",
    "\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 8, 2, 182)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatten confusion matrix and assign labels appropriately \n",
    "#match labels w/ crosstab and array above\n",
    "tp1, fn1, fp1, tn1 = model1.ravel()\n",
    "\n",
    "tp1, fn1, fp1, tn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  9,   7],\n",
       "       [ 81, 103]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#do same step for model 2\n",
    "model2 = confusion_matrix(c3df.actual, \n",
    "                          c3df.model2, \n",
    "                          labels=['Defect', 'No Defect'])\n",
    "\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 7, 81, 103)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp2, fn2, fp2, tn2 = model2.ravel()\n",
    "\n",
    "tp2, fn2, fp2, tn2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  3],\n",
       "       [86, 98]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same for model 3\n",
    "model3 = confusion_matrix(c3df.actual, \n",
    "                          c3df.model3, \n",
    "                          labels=['Defect', 'No Defect'])\n",
    "\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 3, 86, 98)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp3, fn3, fp3, tn3 = model3.ravel()\n",
    "\n",
    "tp3, fn3, fp3, tn3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find evaluation metric that gives the most defects (aka true positive)\n",
    "#want to reduce false negatives (duck is defective and model shows no defect)\n",
    "#Recall optimizes for this (tp / (tp + fn))\n",
    "\n",
    "recall_model1 = tp1 / (tp1 + fn1)\n",
    "recall_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_model2 = tp2 / (tp2 + fn2)\n",
    "recall_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8125"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_model3 = tp3 / (tp3 + fn3)\n",
    "recall_model3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recently several stories in the local news have come out highlighting customers who received a rubber duck with a defect, and portraying C3 in a bad light. The PR team has decided to launch a program that gives customers with a defective duck a vacation to Hawaii. They need you to predict which ducks will have defects, but tell you they really don't want to accidentally give out a vacation package when the duck really doesn't have a defect. \n",
    "    - Which evaluation metric would be appropriate here? \n",
    "        - Precision to minimize false positives\n",
    "    - Which model would be the best fit for this use case?\n",
    "        - Model 1 has highest precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#need to predict ducks w/ defects, \n",
    "#don't want to accidentally predict defect when duck really doesn't have defect\n",
    "#aka don't want false positives\n",
    "#Precision optimizes for this (tp / (tp + fp))\n",
    "\n",
    "precision_model1 = tp1 / (tp1 + fp1)\n",
    "precision_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_model2 = tp2 / (tp2 + fp2)\n",
    "precision_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13131313131313133"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_model3 = tp3 / (tp3 + fp3)\n",
    "precision_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate solution for exercise 3 part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      actual     model1  model2     model3\n",
       "0  No Defect  No Defect  Defect  No Defect\n",
       "1  No Defect  No Defect  Defect     Defect\n",
       "2  No Defect  No Defect  Defect  No Defect\n",
       "3  No Defect     Defect  Defect     Defect\n",
       "4  No Defect  No Defect  Defect  No Defect"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('https://ds.codeup.com/data/c3.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[182,   2],\n",
       "       [  8,   8]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix for model 1\n",
    "confusion_matrix(df.actual, df.model1,\n",
    "                 labels = ['No Defect', 'Defect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[103,  81],\n",
       "       [  7,   9]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix for model 2\n",
    "confusion_matrix(df.actual, df.model2,\n",
    "                 labels = ['No Defect', 'Defect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[98, 86],\n",
       "       [ 3, 13]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#confusion matrix for model 3 \n",
    "confusion_matrix(df.actual, df.model3,\n",
    "                 labels = ['No Defect', 'Defect'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>Defect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Defect</td>\n",
       "      <td>Defect</td>\n",
       "      <td>No Defect</td>\n",
       "      <td>No Defect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actual     model1     model2     model3\n",
       "13   Defect  No Defect     Defect     Defect\n",
       "30   Defect     Defect  No Defect     Defect\n",
       "65   Defect     Defect     Defect     Defect\n",
       "70   Defect     Defect     Defect     Defect\n",
       "74   Defect  No Defect  No Defect     Defect\n",
       "87   Defect  No Defect     Defect     Defect\n",
       "118  Defect  No Defect     Defect  No Defect\n",
       "135  Defect     Defect  No Defect     Defect\n",
       "140  Defect  No Defect     Defect     Defect\n",
       "147  Defect     Defect  No Defect     Defect\n",
       "163  Defect     Defect     Defect     Defect\n",
       "171  Defect  No Defect     Defect     Defect\n",
       "176  Defect  No Defect     Defect     Defect\n",
       "186  Defect  No Defect  No Defect  No Defect\n",
       "194  Defect     Defect  No Defect     Defect\n",
       "196  Defect     Defect  No Defect  No Defect"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1\n",
    "\n",
    "subset = df[df.actual == 'Defect']\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Model recall: 50.00%\n"
     ]
    }
   ],
   "source": [
    "#Model 1 recall\n",
    "\n",
    "model_recall = (subset.actual == subset.model1).mean()\n",
    "print(\"Model 1\")\n",
    "print(f\"Model recall: {model_recall:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n",
      "Model recall: 56.25%\n"
     ]
    }
   ],
   "source": [
    "# Model 2 recall\n",
    "\n",
    "model_recall = (subset.actual == subset.model2).mean()\n",
    "print(\"Model 2\")\n",
    "print(f\"Model recall: {model_recall:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n",
      "Model recall: 81.25%\n"
     ]
    }
   ],
   "source": [
    "# Model 3 recall\n",
    "\n",
    "model_recall = (subset.actual == subset.model3).mean()\n",
    "print(\"Model 3\")\n",
    "print(f\"Model recall: {model_recall:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate solution for exercise 3 part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1\n",
      "Model precision: 80.00%\n"
     ]
    }
   ],
   "source": [
    "subset = df[df.model1 == 'Defect']\n",
    "\n",
    "model_precision = (subset.actual == subset.model1).mean()\n",
    "\n",
    "print(\"Model 1\")\n",
    "print(f\"Model precision: {model_precision:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2\n",
      "Model precision: 10.00%\n"
     ]
    }
   ],
   "source": [
    "subset = df[df.model2 == 'Defect']\n",
    "\n",
    "model_precision = (subset.actual == subset.model2).mean()\n",
    "\n",
    "print(\"Model 2\")\n",
    "print(f\"Model precision: {model_precision:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 3\n",
      "Model precision: 13.13%\n"
     ]
    }
   ],
   "source": [
    "subset = df[df.model3 == 'Defect']\n",
    "\n",
    "model_precision = (subset.actual == subset.model3).mean()\n",
    "\n",
    "print(\"Model 3\")\n",
    "print(f\"Model precision: {model_precision:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. You are working as a data scientist for Gives You Paws ™, a subscription based service that shows you cute pictures of dogs or cats (or both for an additional fee).\n",
    "\n",
    "At Gives You Paws, anyone can upload pictures of their cats or dogs. The photos are then put through a two step process. \n",
    "\n",
    "- First, an automated algorithm tags pictures as either a cat or a dog (Phase I). \n",
    "- Next, the photos that have been initially identified are put through another round of review, possibly with some human oversight, before being presented to the users (Phase II).\n",
    "\n",
    "Several models have already been developed with the data, and you can find their results here. https://ds.codeup.com/data/gives_you_paws.csv\n",
    "\n",
    "Given this dataset, use pandas to create a baseline model (i.e. a model that just predicts the most common class) and answer the following questions:\n",
    "\n",
    "a. In terms of accuracy, how do the various models compare to the baseline model? \n",
    "\n",
    "Are any of the models better than the baseline?\n",
    "- Model 1 and model 4 are more accurate than the baseline model.\n",
    "- Model 2 and model 3 are less accurate than the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4\n",
       "0    cat    cat    dog    cat    dog\n",
       "1    dog    dog    cat    cat    dog\n",
       "2    dog    cat    cat    cat    dog\n",
       "3    dog    dog    dog    cat    dog\n",
       "4    cat    cat    cat    dog    dog"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pawsdf = pd.read_csv('gives_you_paws.csv')\n",
    "pawsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dog    3254\n",
       "cat    1746\n",
       "Name: actual, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dog is most common\n",
    "pawsdf.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>model1</th>\n",
       "      <th>model2</th>\n",
       "      <th>model3</th>\n",
       "      <th>model4</th>\n",
       "      <th>baseline_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  actual model1 model2 model3 model4 baseline_prediction\n",
       "0    cat    cat    dog    cat    dog                 dog\n",
       "1    dog    dog    cat    cat    dog                 dog\n",
       "2    dog    cat    cat    cat    dog                 dog\n",
       "3    dog    dog    dog    cat    dog                 dog\n",
       "4    cat    cat    cat    dog    dog                 dog"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3254 dogs\n",
    "#1746 cats \n",
    "#so our baseline model would be to predict dog every single time.\n",
    "pawsdf['baseline_prediction'] = 'dog'\n",
    "pawsdf.head()\n",
    "\n",
    "#or df[\"baseline\"] = df.actual.value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy: 80.74%\n",
      "model 2 accuracy: 63.04%\n",
      "model 3 accuracy: 50.96%\n",
      "model 4 accuracy: 74.26%\n",
      "baseline accuracy: 65.08%\n"
     ]
    }
   ],
   "source": [
    "#In terms of accuracy, how do the various models compare to the baseline model?\n",
    "#Are any of the models better than the baseline? Yes (model 1 and 4)\n",
    "\n",
    "model1_accuracy = (pawsdf.actual == pawsdf.model1).mean()\n",
    "model2_accuracy = (pawsdf.actual == pawsdf.model2).mean()\n",
    "model3_accuracy = (pawsdf.actual == pawsdf.model3).mean()\n",
    "model4_accuracy = (pawsdf.actual == pawsdf.model4).mean()\n",
    "\n",
    "baseline_accuracy = (pawsdf.actual == pawsdf.baseline_prediction).mean()\n",
    "\n",
    "print(f'model 1 accuracy: {model1_accuracy:.2%}')\n",
    "print(f'model 2 accuracy: {model2_accuracy:.2%}')\n",
    "print(f'model 3 accuracy: {model3_accuracy:.2%}')\n",
    "print(f'model 4 accuracy: {model4_accuracy:.2%}')\n",
    "\n",
    "print(f'baseline accuracy: {baseline_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model1': 0.8074,\n",
       " 'model2': 0.6304,\n",
       " 'model3': 0.5096,\n",
       " 'model4': 0.7426,\n",
       " 'baseline_prediction': 0.6508}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#or make output dictionary\n",
    "\n",
    "#stp 1: get col names into a list\n",
    "models = list(pawsdf.columns)\n",
    "models = models[1:]\n",
    "\n",
    "#stp 2: get accuracy in dictionary form for each model\n",
    "output = {}\n",
    "for model in models:\n",
    "    accuracy = (pawsdf.actual == pawsdf[model]).mean()\n",
    "    output.update({model:accuracy})\n",
    "    \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>model1</td>\n",
       "      <td>0.8074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>model2</td>\n",
       "      <td>0.6304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model3</td>\n",
       "      <td>0.5096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>model4</td>\n",
       "      <td>0.7426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>baseline_prediction</td>\n",
       "      <td>0.6508</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model  accuracy\n",
       "0               model1    0.8074\n",
       "1               model2    0.6304\n",
       "2               model3    0.5096\n",
       "3               model4    0.7426\n",
       "4  baseline_prediction    0.6508"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make into a dataframe\n",
    "pd.DataFrame(list(output.items()), columns = ['model', 'accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Suppose you are working on a team that solely deals with dog pictures. Which of these models would you recomend for Phase I? \n",
    "- Recommend model 4\n",
    "\n",
    "For Phase II?\n",
    "- Recommend model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 recall: 80.33%\n",
      "model 2 recall: 49.08%\n",
      "model 3 recall: 50.86%\n",
      "model 4 recall: 95.57%\n"
     ]
    }
   ],
   "source": [
    "#needing dog pictures (dog = positive, cat = negative)\n",
    "#phase I is automated algorithm tags pics as either cat or dog\n",
    "\n",
    "#need recall to minimize false negatives\n",
    "\n",
    "subset = pawsdf[pawsdf.actual == 'dog']\n",
    "\n",
    "model1_recall = (subset.actual == subset.model1).mean()\n",
    "model2_recall = (subset.actual == subset.model2).mean()\n",
    "model3_recall = (subset.actual == subset.model3).mean()\n",
    "model4_recall = (subset.actual == subset.model4).mean()\n",
    "\n",
    "print(f'model 1 recall: {model1_recall:.2%}')\n",
    "print(f'model 2 recall: {model2_recall:.2%}')\n",
    "print(f'model 3 recall: {model3_recall:.2%}')\n",
    "print(f'model 4 recall: {model4_recall:.2%}')\n",
    "\n",
    "#model 4 is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 precision: 89.00%\n",
      "model 2 precision: 89.32%\n",
      "model 3 precision: 65.99%\n",
      "model 4 precision: 73.12%\n"
     ]
    }
   ],
   "source": [
    "#phase II is photos that have been initially identified are put through another round of review\n",
    "#all features from first model are going to be fed into 2nd model\n",
    "#output photos will be presented to customers, so we don't want false positives\n",
    "#need precision to minimize false positives\n",
    "#filter out for each model all the observations where prediction is dog\n",
    "\n",
    "subset1 = pawsdf[pawsdf.model1 == 'dog']\n",
    "subset2 = pawsdf[pawsdf.model2 == 'dog']\n",
    "subset3 = pawsdf[pawsdf.model3 == 'dog']\n",
    "subset4 = pawsdf[pawsdf.model4 == 'dog']\n",
    "\n",
    "model1_precision = (subset1.actual == subset1.model1).mean()\n",
    "model2_precision = (subset2.actual == subset2.model2).mean()\n",
    "model3_precision = (subset3.actual == subset3.model3).mean()\n",
    "model4_precision = (subset4.actual == subset4.model4).mean()\n",
    "\n",
    "print(f'model 1 precision: {model1_precision:.2%}')\n",
    "print(f'model 2 precision: {model2_precision:.2%}')\n",
    "print(f'model 3 precision: {model3_precision:.2%}')\n",
    "print(f'model 4 precision: {model4_precision:.2%}')\n",
    "\n",
    "#model 2 is best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c. Suppose you are working on a team that solely deals with cat pictures. Which of these models would you recomend for Phase I? \n",
    "\n",
    "For Phase II?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 recall: 81.50%\n",
      "model 2 recall: 89.06%\n",
      "model 3 recall: 51.15%\n",
      "model 4 recall: 34.54%\n"
     ]
    }
   ],
   "source": [
    "#needing cat pictures (cat = positive, dog = negative)\n",
    "#phase I is automated algorithm tags pics as either cat or dog\n",
    "\n",
    "#need recall to minimize false negatives\n",
    "\n",
    "subset = pawsdf[pawsdf.actual == 'cat']\n",
    "\n",
    "model1_recall = (subset.actual == subset.model1).mean()\n",
    "model2_recall = (subset.actual == subset.model2).mean()\n",
    "model3_recall = (subset.actual == subset.model3).mean()\n",
    "model4_recall = (subset.actual == subset.model4).mean()\n",
    "\n",
    "print(f'model 1 recall: {model1_recall:.2%}')\n",
    "print(f'model 2 recall: {model2_recall:.2%}')\n",
    "print(f'model 3 recall: {model3_recall:.2%}')\n",
    "print(f'model 4 recall: {model4_recall:.2%}')\n",
    "\n",
    "#model 2 is best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 precision: 68.98%\n",
      "model 2 precision: 48.41%\n",
      "model 3 precision: 35.83%\n",
      "model 4 precision: 80.72%\n"
     ]
    }
   ],
   "source": [
    "#phase II is photos that have been initially identified are put through another round of review\n",
    "#all features from first model are going to be fed into 2nd model\n",
    "#output photos will be presented to customers, so we don't want false positives\n",
    "#need precision to minimize false positives\n",
    "#filter out for each model all the observations where prediction is cat\n",
    "\n",
    "subset1 = pawsdf[pawsdf.model1 == 'cat']\n",
    "subset2 = pawsdf[pawsdf.model2 == 'cat']\n",
    "subset3 = pawsdf[pawsdf.model3 == 'cat']\n",
    "subset4 = pawsdf[pawsdf.model4 == 'cat']\n",
    "\n",
    "model1_precision = (subset1.actual == subset1.model1).mean()\n",
    "model2_precision = (subset2.actual == subset2.model2).mean()\n",
    "model3_precision = (subset3.actual == subset3.model3).mean()\n",
    "model4_precision = (subset4.actual == subset4.model4).mean()\n",
    "\n",
    "print(f'model 1 precision: {model1_precision:.2%}')\n",
    "print(f'model 2 precision: {model2_precision:.2%}')\n",
    "print(f'model 3 precision: {model3_precision:.2%}')\n",
    "print(f'model 4 precision: {model4_precision:.2%}')\n",
    "\n",
    "#model 4 is best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Follow the links below to read the documentation about each function, then apply those functions to the data from the previous problem.\n",
    "\n",
    "sklearn.metrics.accuracy_score https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html\n",
    "\n",
    "sklearn.metrics.precision_score https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "\n",
    "sklearn.metrics.recall_score https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html\n",
    "\n",
    "sklearn.metrics.classification_report https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 accuracy: 80.74%\n",
      "model 2 accuracy: 63.04%\n",
      "model 3 accuracy: 50.96%\n",
      "model 4 accuracy: 74.26%\n"
     ]
    }
   ],
   "source": [
    "#accuracy from sklearn\n",
    "model1_accuracy = accuracy_score(pawsdf.actual, pawsdf.model1)\n",
    "model2_accuracy = accuracy_score(pawsdf.actual, pawsdf.model2)\n",
    "model3_accuracy = accuracy_score(pawsdf.actual, pawsdf.model3)\n",
    "model4_accuracy = accuracy_score(pawsdf.actual, pawsdf.model4)\n",
    "\n",
    "print(f'model 1 accuracy: {model1_accuracy:.2%}')\n",
    "print(f'model 2 accuracy: {model2_accuracy:.2%}')\n",
    "print(f'model 3 accuracy: {model3_accuracy:.2%}')\n",
    "print(f'model 4 accuracy: {model4_accuracy:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 precision: 78.99%\n",
      "model 2 precision: 68.86%\n",
      "model 3 precision: 50.91%\n",
      "model 4 precision: 76.92%\n"
     ]
    }
   ],
   "source": [
    "#precision from sklearn\n",
    "\n",
    "#average{‘micro’, ‘macro’, ‘samples’, ‘weighted’, ‘binary’} default=’binary’\n",
    "#This parameter is required for multiclass/multilabel targets. \n",
    "#If None, the scores for each class are returned. \n",
    "# Otherwise, this determines the type of averaging performed on the data:\n",
    "#'macro': Calculate metrics for each label, and find their unweighted mean. \n",
    "    #This does not take label imbalance into account.\n",
    "    \n",
    "model1_precision = precision_score(pawsdf.actual, pawsdf.model1, average='macro')\n",
    "model2_precision = precision_score(pawsdf.actual, pawsdf.model2, average='macro')\n",
    "model3_precision = precision_score(pawsdf.actual, pawsdf.model3, average='macro')\n",
    "model4_precision = precision_score(pawsdf.actual, pawsdf.model4, average='macro')\n",
    "\n",
    "print(f'model 1 precision: {model1_precision:.2%}')\n",
    "print(f'model 2 precision: {model2_precision:.2%}')\n",
    "print(f'model 3 precision: {model3_precision:.2%}')\n",
    "print(f'model 4 precision: {model4_precision:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 recall: 80.92%\n",
      "model 2 recall: 69.07%\n",
      "model 3 recall: 51.00%\n",
      "model 4 recall: 65.06%\n"
     ]
    }
   ],
   "source": [
    "#recall from sklearn\n",
    "\n",
    "model1_recall = recall_score(pawsdf.actual, pawsdf.model1, average='macro')\n",
    "model2_recall = recall_score(pawsdf.actual, pawsdf.model2, average='macro')\n",
    "model3_recall = recall_score(pawsdf.actual, pawsdf.model3, average='macro')\n",
    "model4_recall = recall_score(pawsdf.actual, pawsdf.model4, average='macro')\n",
    "\n",
    "print(f'model 1 recall: {model1_recall:.2%}')\n",
    "print(f'model 2 recall: {model2_recall:.2%}')\n",
    "print(f'model 3 recall: {model3_recall:.2%}')\n",
    "print(f'model 4 recall: {model4_recall:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 1 classification report\n",
      "-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.69      0.82      0.75      1746\n",
      "         dog       0.89      0.80      0.84      3254\n",
      "\n",
      "    accuracy                           0.81      5000\n",
      "   macro avg       0.79      0.81      0.80      5000\n",
      "weighted avg       0.82      0.81      0.81      5000\n",
      "\n",
      "-----------------------------------------------------\n",
      " \n",
      " \n",
      " \n",
      "model 2 classification report\n",
      "-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.48      0.89      0.63      1746\n",
      "         dog       0.89      0.49      0.63      3254\n",
      "\n",
      "    accuracy                           0.63      5000\n",
      "   macro avg       0.69      0.69      0.63      5000\n",
      "weighted avg       0.75      0.63      0.63      5000\n",
      "\n",
      "-----------------------------------------------------\n",
      " \n",
      " \n",
      " \n",
      "model 3 classification report\n",
      "-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.36      0.51      0.42      1746\n",
      "         dog       0.66      0.51      0.57      3254\n",
      "\n",
      "    accuracy                           0.51      5000\n",
      "   macro avg       0.51      0.51      0.50      5000\n",
      "weighted avg       0.55      0.51      0.52      5000\n",
      "\n",
      "-----------------------------------------------------\n",
      " \n",
      " \n",
      " \n",
      "model 4 classification report\n",
      "-----------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         cat       0.81      0.35      0.48      1746\n",
      "         dog       0.73      0.96      0.83      3254\n",
      "\n",
      "    accuracy                           0.74      5000\n",
      "   macro avg       0.77      0.65      0.66      5000\n",
      "weighted avg       0.76      0.74      0.71      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#classification report from sklearn\n",
    "\n",
    "model1_classreport = classification_report(pawsdf.actual, pawsdf.model1)\n",
    "model2_classreport = classification_report(pawsdf.actual, pawsdf.model2)\n",
    "model3_classreport = classification_report(pawsdf.actual, pawsdf.model3)\n",
    "model4_classreport = classification_report(pawsdf.actual, pawsdf.model4)\n",
    "\n",
    "print(\"model 1 classification report\")\n",
    "print(\"-----------------------------\")\n",
    "print(model1_classreport)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"model 2 classification report\")\n",
    "print(\"-----------------------------\")\n",
    "print(model2_classreport)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"model 3 classification report\")\n",
    "print(\"-----------------------------\")\n",
    "print(model3_classreport)\n",
    "print(\"-----------------------------------------------------\")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "\n",
    "print(\"model 4 classification report\")\n",
    "print(\"-----------------------------\")\n",
    "print(model4_classreport)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
